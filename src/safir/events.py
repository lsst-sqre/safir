"""Utilities for integrating with SQuaRE Events (Kafka)."""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING

import structlog
from aiokafka import AIOKafkaProducer
from kafkit.registry.aiohttp import RegistryApi
from kafkit.registry.manager import RecordNameSchemaManager
from kafkit.ssl import create_ssl_context

__all__ = [
    "configure_kafka_ssl",
    "init_kafka_producer",
    "init_recordname_schema_manager",
]

if TYPE_CHECKING:
    from typing import AsyncGenerator
    from aiohttp.web import Application


async def configure_kafka_ssl(app: Application) -> AsyncGenerator:
    """Configure an SSL context for the Kafka client (if appropriate).

    Parameters
    ----------
    app : `aiohttp.web.Application`
        The aiohttp.web-based application. This app *must* include a standard
        configuration object at the ``"safir/config"`` key. The config must
        have these attributes:

        ``logger_name``
            Name of the application's logger.
        ``kafka_protocol``
            The protocol for Kafka broker communication. Must be ``"SSL"``
            to make the SSL context (this function is a no-op otherwise).
        ``kafka_cluster_ca_path``
            Local file path of the Kafka cluster CA.
        ``kafka_client_cert_path``
            Local file path of the Kafka client cert.
        ``kafka_client_key_path``
            Local file path of the Kafka client key.

    Notes
    -----
    If the ``config.kafka_protocol`` configuration is ``SSL``, this function
    sets up an SSL context (`ssl.create_default_context) using the certificates
    generated by Strimzi for the Kafka brokers and for this specific client.

    The SSL context is stored on the application under the
    ``safir/kafka_ssl_context`` key.

    Examples
    --------
    Use this function as a `cleanup context
    <https://aiohttp.readthedocs.io/en/stable/web_reference.html#aiohttp.web.Application.cleanup_ctx>`__:

    .. code-block:: python

       app.cleanup_ctx.append(configure_kafka_ssl)
    """
    logger = structlog.get_logger(app["safir/config"].logger_name)

    ssl_context_key = "safir/kafka_ssl_context"

    if app["safir/config"].kafka_protocol != "SSL":
        app[ssl_context_key] = None
        logger.info("Connecting to Kafka without SSL")

    else:
        ssl_context = create_ssl_context(
            cluster_ca_path=app["safir/config"].kafka_cluster_ca_path,
            client_cert_path=app["safir/config"].kafka_client_cert_path,
            client_key_path=app["safir/config"].kafka_client_key_path,
        )

        app[ssl_context_key] = ssl_context

        logger.info("Created Kafka SSL context")

    yield


async def init_kafka_producer(app: Application) -> AsyncGenerator:
    """Initialize and cleanup the aiokafka producer instance.

    Parameters
    ----------
    app : `aiohttp.web.Application`
        The aiohttp.web-based application. This app *must* include a standard
        configuration object at the ``"safir/config"`` key. The config must
        have these attributes:

        ``logger_name``
            Name of the application's logger.
        ``kafka_broker_url``
            The URL of a Kafka broker.
        ``kafka_protocol``
            The protocol for Kafka broker communication.

        Additionally, `configure_kafka_ssl` must be applied **before** this
        initializer so that ``safir/kafka_ssl_context`` is set on the
        application.

    Notes
    -----
    This initializer adds an `aiokafka.AIOKafkaProducer` instance to the
    ``app`` under the ``safir/kafka_producer`` key.

    If the ``kafka_broker_url`` configuration key has a value of `None`, then
    the value of ``safir/kafka_producer`` is `None`.

    Examples
    --------
    Use this function as a `cleanup context
    <https://aiohttp.readthedocs.io/en/stable/web_reference.html#aiohttp.web.Application.cleanup_ctx>`__.

    To access the producer:

    .. code-block:: python

       producer = app["safir/kafka_producer"]
    """
    logger = structlog.get_logger(app["safir/config"].logger_name)

    # Startup phase
    kafka_broker_url = app["safir/config"].kafka_broker_url

    if kafka_broker_url is None:
        logger.info("Skipping Kafka producer initialization")
        producer = None

    else:
        logger.info("Starting Kafka producer")
        producer = AIOKafkaProducer(
            loop=asyncio.get_running_loop(),
            bootstrap_servers=kafka_broker_url,
            ssl_context=app["safir/kafka_ssl_context"],
            security_protocol=app["safir/config"].kafka_protocol,
        )
        await producer.start()
        app["safir/kafka_producer"] = producer
        logger.info("Finished starting Kafka producer")

    yield

    # Cleanup phase
    if producer is not None:
        logger.info("Shutting down Kafka producer")
        await producer.stop()


async def init_recordname_schema_manager(app: Application) -> AsyncGenerator:
    """Initialize a `~kafkit.registry.manager.RecordNameSchemaManager` for
    serializing messages given the name of the Avro schema.

    Parameters
    ----------
    app : `aiohttp.web.Application`
        The aiohttp.web-based application. This app *must* include a standard
        configuration object at the ``"safir/config"`` key. The config must
        have these attributes:

        ``logger_name``
            Name of the application's logger (`str`).

        ``schema_registry_url``
            The URL of a Schema Registry (`str`).

        ``schema_compatibility``
            The compatibility setting to apply to schemas in a Schema Registry
            subject.

            Valid settings are:

            - ``"BACKWARD"``
            - ``"BACKWARD_TRANSITIVE"``
            - ``"FORWARD"``
            - ``"FORWARD_TRANSITIVE"``
            - ``"FULL"``
            - ``"FULL_TRANSITIVE"``
            - ``"NONE"``

            Leave as `None` to avoid setting (or resetting) a subject.

        ``schema_root_dir``
            A local file path (`pathlib.Path`) for a directory containing
            Avro schemas.

        ``schema_suffix``
            A suffix to apply to all locally-managed Avro schema names. The
            suffix allows you dynamically create alternative Schema Registry
            subjects for development.

        Additionally, the `safir.http.init_http_session` initializer needs
        to be run **before** this initializer. The ``safir/http_session``
        key needs to be available on the ``app``.

    Notes
    -----
    This initializer adds a `~kafkit.registry.manager.RecordNameSchemaManager`
    to the ``app`` under the ``safir/schema_manager`` key.

    Examples
    --------
    Use this function as a `cleanup context
    <https://aiohttp.readthedocs.io/en/stable/web_reference.html#aiohttp.web.Application.cleanup_ctx>`__.

    To access the schema manager:

    .. code-block:: python

       manager = app["safir/schema_manager"]
    """
    logger = structlog.get_logger(app["safir/config"].logger_name)

    registry_url = app["safir/config"].schema_registry_url
    if registry_url is None:
        raise RuntimeError(
            "The `schema_registry_url` configuration attribute must be set "
            "for init_recordname_schema_manager."
        )

    registry = RegistryApi(
        session=app["safir/http_session"],
        url=app["safir/config"].schema_registry_url,
    )

    schema_root_dir = app["safir/config"].schema_root_dir

    try:
        schema_suffix = app["safir/config"].schema_suffix
    except AttributeError:
        schema_suffix = ""

    manager = RecordNameSchemaManager(
        root=schema_root_dir, suffix=schema_suffix, registry=registry,
    )
    await manager.register_schemas(
        compatibility=app["safir/config"].schema_compatibility
    )

    app["safir/schema_registry"] = registry
    app["safir/schema_manager"] = manager
    logger.info("Finished registering Avro schemas")

    yield
